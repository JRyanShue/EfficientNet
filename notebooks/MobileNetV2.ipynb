{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MobileNetV2.ipynb","provenance":[],"authorship_tag":"ABX9TyNTTtGT/5356qXV/Ef69Xjb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2daadd2eff42428abb24e9ab5dfa41a0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a2d23acd505b4832a0173365ef37eb13","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7c04124f6af54ad4b57484f7e12ff2ba","IPY_MODEL_5a26ae41862f4863a627498d6fc2daec","IPY_MODEL_5dc951e089614193912fd7795e827285"]}},"a2d23acd505b4832a0173365ef37eb13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7c04124f6af54ad4b57484f7e12ff2ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_28259fd630d94633b57f131f514cfd3b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5be8afd31e7c4ec7bedd3d1f2814a4f7"}},"5a26ae41862f4863a627498d6fc2daec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_146b69b8d22d47729fc8f21b7e104deb","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6b7a1843209c44a3a49231e1ce24b443"}},"5dc951e089614193912fd7795e827285":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e1012ad4d86e4b91bf325a26537e954b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:00&lt;00:00,  5.72it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_422e0849c94441cabb14ddee2b8d2aab"}},"28259fd630d94633b57f131f514cfd3b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5be8afd31e7c4ec7bedd3d1f2814a4f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"146b69b8d22d47729fc8f21b7e104deb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6b7a1843209c44a3a49231e1ce24b443":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e1012ad4d86e4b91bf325a26537e954b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"422e0849c94441cabb14ddee2b8d2aab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cwwf7mXR8rI9","executionInfo":{"status":"ok","timestamp":1639326392373,"user_tz":300,"elapsed":8221,"user":{"displayName":"Ryan Shue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg65rZPkz5bJ7ik0h6ITKz45vgUzj81potiaE77Dw=s64","userId":"11228421312565688614"}},"outputId":"7eaaa9d7-351a-4ba2-d9e3-bd9fc06210a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.16.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.2.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.11.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.1)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.8)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (5.2.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.7.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.22.0)\n","Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n","Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (12.0.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n","Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n","Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.42.0)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n"]}],"source":["!pip install datasets\n","!pip install tensorflow"]},{"cell_type":"code","source":["\n","from datasets import load_dataset\n","\n","dataset = load_dataset('cifar10')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86,"referenced_widgets":["2daadd2eff42428abb24e9ab5dfa41a0","a2d23acd505b4832a0173365ef37eb13","7c04124f6af54ad4b57484f7e12ff2ba","5a26ae41862f4863a627498d6fc2daec","5dc951e089614193912fd7795e827285","28259fd630d94633b57f131f514cfd3b","5be8afd31e7c4ec7bedd3d1f2814a4f7","146b69b8d22d47729fc8f21b7e104deb","6b7a1843209c44a3a49231e1ce24b443","e1012ad4d86e4b91bf325a26537e954b","422e0849c94441cabb14ddee2b8d2aab"]},"id":"RDenx4OL9AJ2","executionInfo":{"status":"ok","timestamp":1639326399524,"user_tz":300,"elapsed":7164,"user":{"displayName":"Ryan Shue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg65rZPkz5bJ7ik0h6ITKz45vgUzj81potiaE77Dw=s64","userId":"11228421312565688614"}},"outputId":"bae3a671-b2ca-4995-e555-ba67c3d681c6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["Reusing dataset cifar10 (/root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/5da9550526dac91579c0df95a56466f78e62cc6ea1ccffd17f71f2e64aa86b5e)\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2daadd2eff42428abb24e9ab5dfa41a0","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{}}]},{"cell_type":"code","source":["print(dataset['train'].shape)\n","dataset['test'].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VMa3VqhM9nFA","executionInfo":{"status":"ok","timestamp":1639326399525,"user_tz":300,"elapsed":14,"user":{"displayName":"Ryan Shue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg65rZPkz5bJ7ik0h6ITKz45vgUzj81potiaE77Dw=s64","userId":"11228421312565688614"}},"outputId":"340b2db0-044a-4d65-a1ae-241aee3b4fdb"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["(50000, 2)\n"]},{"output_type":"execute_result","data":{"text/plain":["(10000, 2)"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["dataset['train'].features"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0pTiVYew_IqI","executionInfo":{"status":"ok","timestamp":1639326399525,"user_tz":300,"elapsed":11,"user":{"displayName":"Ryan Shue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg65rZPkz5bJ7ik0h6ITKz45vgUzj81potiaE77Dw=s64","userId":"11228421312565688614"}},"outputId":"6a15fbbe-4159-4950-f47f-49b1c6193c25"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'img': Array3D(shape=(32, 32, 3), dtype='uint8', id=None),\n"," 'label': ClassLabel(num_classes=10, names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'], names_file=None, id=None)}"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["dataset['train']['label'][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sNX9X6bt_VTY","executionInfo":{"status":"ok","timestamp":1639326399528,"user_tz":300,"elapsed":11,"user":{"displayName":"Ryan Shue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg65rZPkz5bJ7ik0h6ITKz45vgUzj81potiaE77Dw=s64","userId":"11228421312565688614"}},"outputId":"4b00da2e-a9e1-4856-901f-cc3562dd623e"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["import tensorflow as tf"],"metadata":{"id":"HGHkyjyC_i-G","executionInfo":{"status":"ok","timestamp":1639326405236,"user_tz":300,"elapsed":5718,"user":{"displayName":"Ryan Shue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg65rZPkz5bJ7ik0h6ITKz45vgUzj81potiaE77Dw=s64","userId":"11228421312565688614"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# Convert dataset into ndarrays\n","\n","(train_x, train_y), (test_x, test_y) = \\\n","  (np.array(dataset['train']['img']), np.array(dataset['train']['label'])), \\\n","  (np.array(dataset['test']['img']), np.array(dataset['test']['label']))\n","\n","# Scale input image pixel values\n","\n","train_x, test_x = train_x / 255.0, test_x / 255.0"],"metadata":{"id":"BE4Rh5-EAf4L","executionInfo":{"status":"ok","timestamp":1639326513312,"user_tz":300,"elapsed":108090,"user":{"displayName":"Ryan Shue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg65rZPkz5bJ7ik0h6ITKz45vgUzj81potiaE77Dw=s64","userId":"11228421312565688614"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["train_x[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X1Y_koAJCqMp","executionInfo":{"status":"ok","timestamp":1639326513313,"user_tz":300,"elapsed":16,"user":{"displayName":"Ryan Shue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg65rZPkz5bJ7ik0h6ITKz45vgUzj81potiaE77Dw=s64","userId":"11228421312565688614"}},"outputId":"2dbd458d-6c52-4da3-fbd9-32d142ef1bf4"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0.69803922, 0.69019608, 0.74117647],\n","        [0.69803922, 0.69019608, 0.74117647],\n","        [0.69803922, 0.69019608, 0.74117647],\n","        ...,\n","        [0.66666667, 0.65882353, 0.70588235],\n","        [0.65882353, 0.65098039, 0.69411765],\n","        [0.64705882, 0.63921569, 0.68235294]],\n","\n","       [[0.70588235, 0.69803922, 0.74901961],\n","        [0.70196078, 0.69411765, 0.74509804],\n","        [0.70588235, 0.69803922, 0.74901961],\n","        ...,\n","        [0.67843137, 0.67058824, 0.71372549],\n","        [0.67058824, 0.6627451 , 0.70588235],\n","        [0.65882353, 0.65098039, 0.69411765]],\n","\n","       [[0.69411765, 0.68627451, 0.7372549 ],\n","        [0.69411765, 0.68627451, 0.7372549 ],\n","        [0.69803922, 0.69019608, 0.74117647],\n","        ...,\n","        [0.67058824, 0.6627451 , 0.70588235],\n","        [0.6627451 , 0.65490196, 0.69803922],\n","        [0.65490196, 0.64705882, 0.69019608]],\n","\n","       ...,\n","\n","       [[0.43921569, 0.41960784, 0.41960784],\n","        [0.44313725, 0.42745098, 0.42352941],\n","        [0.44705882, 0.43137255, 0.43137255],\n","        ...,\n","        [0.39215686, 0.38039216, 0.36862745],\n","        [0.38431373, 0.36862745, 0.36470588],\n","        [0.39607843, 0.37254902, 0.37254902]],\n","\n","       [[0.43921569, 0.4       , 0.39607843],\n","        [0.43921569, 0.40392157, 0.4       ],\n","        [0.44313725, 0.40392157, 0.40392157],\n","        ...,\n","        [0.4       , 0.37254902, 0.36470588],\n","        [0.4       , 0.36470588, 0.35686275],\n","        [0.4       , 0.36078431, 0.35686275]],\n","\n","       [[0.40392157, 0.37647059, 0.36078431],\n","        [0.39215686, 0.36470588, 0.35294118],\n","        [0.40392157, 0.37254902, 0.36862745],\n","        ...,\n","        [0.36078431, 0.32941176, 0.31372549],\n","        [0.36470588, 0.3372549 , 0.31372549],\n","        [0.35686275, 0.32941176, 0.30196078]]])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["import os\n","\n","BATCH_SIZE = 96\n","DROPOUT_RATE = 0.1\n","\n","def train_mobilenetv2(training=True):\n","\n","  # Define callbacks for early training stopping and model checkpointing\n","\n","  class Callback(tf.keras.callbacks.Callback):\n","    def on_epoch_end(self, epoch, logs={}):\n","      if logs.get('acc') > 0.3:\n","        print(\"\\nReached stopping point accuracy.\")\n","        self.model.stop_training = True\n","\n","  early_stop_callback = Callback()\n","\n","  checkpoint_path = \"mobileNetV2/model.ckpt\"\n","  checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","  checkpointing_callback = tf.keras.callbacks.ModelCheckpoint(\n","      filepath=checkpoint_path, save_weights_only=True, verbose=1)\n","\n","  # Define MobileNetV2\n","\n","  # Initial 32-output channel convolution\n","  class initial_conv2d(tf.keras.Sequential):\n","    def __init__(self):\n","      tf.keras.Sequential.__init__(self, layers=[\n","        tf.keras.layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same', \n","                               activation=None, name='i_conv'),\n","        tf.keras.layers.BatchNormalization(momentum=0.999, name='i_bnorm'),\n","        tf.keras.layers.ReLU(max_value=6.0, name='i_relu'),\n","        # Dropout only during training\n","        # tf.keras.layers.Dropout(DROPOUT_RATE)\n","        ])\n","      \n","  # Final 1280-output channel convolution\n","  class final_conv2d(tf.keras.Sequential):\n","    def __init__(self):\n","      tf.keras.Sequential.__init__(self, layers=[\n","        tf.keras.layers.Conv2D(1280, (1, 1),\n","                               activation=None, name='f_conv'),\n","        tf.keras.layers.BatchNormalization(momentum=0.999, name='f_bnorm'),\n","        tf.keras.layers.ReLU(max_value=6.0, name='f_relu'),\n","        # Dropout only during training\n","        # tf.keras.layers.Dropout(DROPOUT_RATE)\n","        ])\n","      \n","  # MBConv inverted residual bottleneck layer\n","  def mbconv2d(input, expansion_factor=1, \n","                 output_channels=16, stride=(1, 1), block_id=1):\n","      \n","      input_channels = input.shape[3]\n","\n","      # Initial 1x1 convolution, multiplying the channel count by the \n","      # expansion factor, widening the representation\n","      conv1 = tf.keras.layers.Conv2D(expansion_factor*input_channels, (1, 1), \n","                              activation=None)(input)\n","      conv1 = tf.keras.layers.BatchNormalization(momentum=0.999)(conv1)\n","      conv1 = tf.keras.layers.ReLU(max_value=6.0)(conv1)\n","      # Dropout only during training\n","      # conv1 = tf.keras.layers.Dropout(DROPOUT_RATE)(conv1)\n","      \n","      # Subsequent 3x3 Depthwise convolution, applying an independent kernel\n","      # For each input channel. Stride automatically performs downsampling.\n","      dwconv1 = tf.keras.layers.DepthwiseConv2D((3, 3), stride, padding='same', \n","                              activation='relu6')(conv1)\n","      dwconv1 = tf.keras.layers.BatchNormalization(momentum=0.999)(dwconv1)\n","      dwconv1 = tf.keras.layers.ReLU(max_value=6.0)(dwconv1)\n","      # Dropout only during training\n","      # dwconv1 = tf.keras.layers.Dropout(DROPOUT_RATE)(dwconv1)\n","      \n","      # Final 1x1 convolution, compressing the channel space into the output\n","      # Channel dimension. No nonlinear activation!\n","      conv2 = tf.keras.layers.Conv2D(output_channels, (1, 1),\n","                              activation=None)(dwconv1)\n","      conv2 = tf.keras.layers.BatchNormalization(momentum=0.999)(conv2)\n","      # Dropout only during training\n","      # conv2 = tf.keras.layers.Dropout(DROPOUT_RATE)(conv2)\n","\n","      # Residual connection\n","\n","      # As in ResNet, if the input and output dimensions to not line up, use\n","      # A linear transformation to map the identity into the output space\n","      # if input.shape != conv2.shape:\n","      #   input = tf.keras.layers.Conv2D(conv2.shape[3], (1, 1), \n","      #       activation=None, name='IdentityConv_' + block_id)(input)\n","      #   input = tf.keras.layers.MaxPooling2D(pool_size=(\n","      #       input.shape[1] / conv2.shape[1], input.shape[2] / \n","      #       conv2.shape[2]), name='IdentityMaxpool_' + block_id)(input)\n","      # return tf.keras.layers.Add()([input, conv2])\n","\n","      # Or, just don't do the residual connection if input and output dims don't\n","      # Match up. (This is how the official implementation is done)\n","      if input_channels == output_channels and stride == (1, 1):\n","        return tf.keras.layers.Add(name='Residual_'+block_id)([input, conv2])\n","      else:\n","        return conv2\n","\n","\n","  # Input shape of CIFAR-10 image\n","  inputs = tf.keras.layers.Input(shape=(32, 32, 3))\n","\n","  # Initial 32-output channel convolution\n","  conv1 = initial_conv2d()(inputs)\n","\n","  # First MBConv series: (16, 16, 32) -> (16, 16, 16); t=1, c=16, n=1, s=1\n","  b1a = mbconv2d(conv1, expansion_factor=1, output_channels=16, \n","                   stride=(1, 1), block_id='1a')\n","  \n","  # Second MBConv series: (16, 16, 16) -> (8, 8, 24); t=6, c=24, n=2, s=2\n","  b2a = mbconv2d(b1a, expansion_factor=6, output_channels=24, \n","                   stride=(2, 2), block_id='2a')\n","  b2b = mbconv2d(b2a, expansion_factor=6, output_channels=24, \n","                   stride=(1, 1), block_id='2b')\n","  \n","  # Third MBConv series: (8, 8, 24) -> (4, 4, 32); t=6, c=32, n=3, s=2\n","  b3a = mbconv2d(b2b, expansion_factor=6, output_channels=32, \n","                   stride=(2, 2), block_id='3a')\n","  b3b = mbconv2d(b3a, expansion_factor=6, output_channels=32, \n","                   stride=(1, 1), block_id='3b')\n","  b3c = mbconv2d(b3b, expansion_factor=6, output_channels=32, \n","                   stride=(1, 1), block_id='3c')\n","  \n","  # Fourth MBConv series: (4, 4, 32) -> (2, 2, 64); t=6, c=64, n=4, s=2\n","  b4a = mbconv2d(b3c, expansion_factor=6, output_channels=64, \n","                   stride=(2, 2), block_id='4a')\n","  b4b = mbconv2d(b4a, expansion_factor=6, output_channels=64, \n","                   stride=(1, 1), block_id='4b')\n","  b4c = mbconv2d(b4b, expansion_factor=6, output_channels=64, \n","                   stride=(1, 1), block_id='4c')\n","  b4d = mbconv2d(b4c, expansion_factor=6, output_channels=64, \n","                   stride=(1, 1), block_id='4d')\n","  # Dropout locations taken from https://github.com/yumaloop/mobilenetV2-cifar/blob/master/models/mobilenet_v2.py\n","  b4d = tf.keras.layers.Dropout(DROPOUT_RATE)(b4d)\n","  \n","  # Fifth MBConv series: (2, 2, 64) -> (2, 2, 96); t=6, c=96, n=3, s=1\n","  b5a = mbconv2d(b4d, expansion_factor=6, output_channels=96, \n","                   stride=(1, 1), block_id='5a')\n","  b5b = mbconv2d(b5a, expansion_factor=6, output_channels=96, \n","                   stride=(1, 1), block_id='5b')\n","  b5c = mbconv2d(b5b, expansion_factor=6, output_channels=96, \n","                   stride=(1, 1), block_id='5c')\n","  b5c = tf.keras.layers.Dropout(DROPOUT_RATE)(b5c)\n","  \n","  # Sixth MBConv series: (2, 2, 96) -> (1, 1, 160); t=6, c=160, n=3, s=2\n","  b6a = mbconv2d(b4d, expansion_factor=6, output_channels=160, \n","                   stride=(1, 1), block_id='6a')\n","  b6b = mbconv2d(b5a, expansion_factor=6, output_channels=160, \n","                   stride=(1, 1), block_id='6b')\n","  b6c = mbconv2d(b5b, expansion_factor=6, output_channels=160, \n","                   stride=(1, 1), block_id='6c')\n","  b6c = tf.keras.layers.Dropout(DROPOUT_RATE)(b6c)\n","  \n","  # Seventh MBConv series: (1, 1, 160) -> (1, 1, 320); t=6, c=320, n=3, s=1\n","  b7a = mbconv2d(b4d, expansion_factor=6, output_channels=320, \n","                   stride=(1, 1), block_id='7a')\n","  b7b = mbconv2d(b5a, expansion_factor=6, output_channels=320, \n","                   stride=(1, 1), block_id='7b')\n","  b7c = mbconv2d(b5b, expansion_factor=6, output_channels=320, \n","                   stride=(1, 1), block_id='7c')\n","  b7c = tf.keras.layers.Dropout(DROPOUT_RATE)(b7c)\n","  \n","  # Final convolution\n","  conv2 = final_conv2d()(b7c)\n","\n","  # Dense layer and softmax\n","  flatten = tf.keras.layers.Flatten()(conv2)\n","  d2 = tf.keras.layers.Dense(10, activation=None)(flatten)\n","  d2 = tf.keras.layers.BatchNormalization(momentum=0.999)(d2)\n","  d2 = tf.keras.layers.Softmax()(d2)\n","\n","  model = tf.keras.Model(inputs=inputs, outputs=d2)\n","\n","  schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n","    0.045, 50000/BATCH_SIZE, 0.98, staircase=False, name='ExpDecaySchedule'\n","  )\n","  # optimizer = tf.keras.optimizers.RMSprop(learning_rate=schedule, rho=0.9)\n","  optimizer = tf.keras.optimizers.Adam(learning_rate=schedule)\n","\n","  # Add weight decay of 0.00004 to all depthwise conv and conv2d layers\n","  for layer in model.layers:\n","    for attr in ['depthwise_regularizer', 'kernel_regularizer']:\n","      if hasattr(layer, attr):\n","        setattr(layer, attr, tf.keras.regularizers.L2(4e-5))\n","        print(layer.get_config())\n","\n","  model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', \n","                metrics=['accuracy'])\n","  \n","  model.summary()\n","\n","  history = model.fit(train_x, train_y, epochs=300, callbacks=[\n","    # early_stop_callback, checkpointing_callback])\n","    checkpointing_callback], batch_size=BATCH_SIZE, \n","    validation_data=(test_x, test_y))\n","\n","  # Return metrics\n","  return history.epoch, history.history['accuracy'][-1]\n"],"metadata":{"id":"snYhKu1OATD_","executionInfo":{"status":"ok","timestamp":1639328591605,"user_tz":300,"elapsed":328,"user":{"displayName":"Ryan Shue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg65rZPkz5bJ7ik0h6ITKz45vgUzj81potiaE77Dw=s64","userId":"11228421312565688614"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["_, _ = train_mobilenetv2()"],"metadata":{"id":"uqklmge7EvNz","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d11d29ae-ae3e-41f7-a674-bbabfbda11ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'name': 'conv2d_114', 'trainable': True, 'dtype': 'float32', 'filters': 32, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'depthwise_conv2d_57', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'depthwise_conv2d_57', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'conv2d_115', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'conv2d_116', 'trainable': True, 'dtype': 'float32', 'filters': 96, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'depthwise_conv2d_58', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (2, 2), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'depthwise_conv2d_58', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (2, 2), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'conv2d_117', 'trainable': True, 'dtype': 'float32', 'filters': 24, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'conv2d_118', 'trainable': True, 'dtype': 'float32', 'filters': 144, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'depthwise_conv2d_59', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'depthwise_conv2d_59', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'conv2d_119', 'trainable': True, 'dtype': 'float32', 'filters': 24, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'conv2d_120', 'trainable': True, 'dtype': 'float32', 'filters': 144, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'depthwise_conv2d_60', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (2, 2), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'depthwise_conv2d_60', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (2, 2), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'conv2d_121', 'trainable': True, 'dtype': 'float32', 'filters': 32, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'conv2d_122', 'trainable': True, 'dtype': 'float32', 'filters': 192, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'depthwise_conv2d_61', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'depthwise_conv2d_61', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'conv2d_123', 'trainable': True, 'dtype': 'float32', 'filters': 32, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'conv2d_124', 'trainable': True, 'dtype': 'float32', 'filters': 192, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'depthwise_conv2d_62', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'depthwise_conv2d_62', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'conv2d_125', 'trainable': True, 'dtype': 'float32', 'filters': 32, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'conv2d_126', 'trainable': True, 'dtype': 'float32', 'filters': 192, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'depthwise_conv2d_63', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (2, 2), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'depthwise_conv2d_63', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (2, 2), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'conv2d_127', 'trainable': True, 'dtype': 'float32', 'filters': 64, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'conv2d_128', 'trainable': True, 'dtype': 'float32', 'filters': 384, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'depthwise_conv2d_64', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'depthwise_conv2d_64', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'conv2d_129', 'trainable': True, 'dtype': 'float32', 'filters': 64, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'conv2d_130', 'trainable': True, 'dtype': 'float32', 'filters': 384, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'depthwise_conv2d_65', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'depthwise_conv2d_65', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'conv2d_131', 'trainable': True, 'dtype': 'float32', 'filters': 64, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'conv2d_132', 'trainable': True, 'dtype': 'float32', 'filters': 384, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'depthwise_conv2d_66', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'depthwise_conv2d_66', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'conv2d_133', 'trainable': True, 'dtype': 'float32', 'filters': 64, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'conv2d_134', 'trainable': True, 'dtype': 'float32', 'filters': 384, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'depthwise_conv2d_67', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'depthwise_conv2d_67', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'conv2d_135', 'trainable': True, 'dtype': 'float32', 'filters': 96, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'conv2d_136', 'trainable': True, 'dtype': 'float32', 'filters': 576, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'depthwise_conv2d_68', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'depthwise_conv2d_68', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'conv2d_137', 'trainable': True, 'dtype': 'float32', 'filters': 96, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'conv2d_150', 'trainable': True, 'dtype': 'float32', 'filters': 576, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'depthwise_conv2d_75', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'depthwise_conv2d_75', 'trainable': True, 'dtype': 'float32', 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu6', 'use_bias': True, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'depthwise_constraint': None}\n","{'name': 'conv2d_151', 'trainable': True, 'dtype': 'float32', 'filters': 320, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","{'name': 'dense_3', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 3.9999998989515007e-05}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n","Model: \"model_3\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_4 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n","                                                                                                  \n"," initial_conv2d_3 (initial_conv  (None, 16, 16, 32)  1024        ['input_4[0][0]']                \n"," 2d)                                                                                              \n","                                                                                                  \n"," conv2d_114 (Conv2D)            (None, 16, 16, 32)   1056        ['initial_conv2d_3[0][0]']       \n","                                                                                                  \n"," batch_normalization_174 (Batch  (None, 16, 16, 32)  128         ['conv2d_114[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_114 (ReLU)               (None, 16, 16, 32)   0           ['batch_normalization_174[0][0]']\n","                                                                                                  \n"," depthwise_conv2d_57 (Depthwise  (None, 16, 16, 32)  320         ['re_lu_114[0][0]']              \n"," Conv2D)                                                                                          \n","                                                                                                  \n"," batch_normalization_175 (Batch  (None, 16, 16, 32)  128         ['depthwise_conv2d_57[0][0]']    \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_115 (ReLU)               (None, 16, 16, 32)   0           ['batch_normalization_175[0][0]']\n","                                                                                                  \n"," conv2d_115 (Conv2D)            (None, 16, 16, 16)   528         ['re_lu_115[0][0]']              \n","                                                                                                  \n"," batch_normalization_176 (Batch  (None, 16, 16, 16)  64          ['conv2d_115[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," conv2d_116 (Conv2D)            (None, 16, 16, 96)   1632        ['batch_normalization_176[0][0]']\n","                                                                                                  \n"," batch_normalization_177 (Batch  (None, 16, 16, 96)  384         ['conv2d_116[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_116 (ReLU)               (None, 16, 16, 96)   0           ['batch_normalization_177[0][0]']\n","                                                                                                  \n"," depthwise_conv2d_58 (Depthwise  (None, 8, 8, 96)    960         ['re_lu_116[0][0]']              \n"," Conv2D)                                                                                          \n","                                                                                                  \n"," batch_normalization_178 (Batch  (None, 8, 8, 96)    384         ['depthwise_conv2d_58[0][0]']    \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_117 (ReLU)               (None, 8, 8, 96)     0           ['batch_normalization_178[0][0]']\n","                                                                                                  \n"," conv2d_117 (Conv2D)            (None, 8, 8, 24)     2328        ['re_lu_117[0][0]']              \n","                                                                                                  \n"," batch_normalization_179 (Batch  (None, 8, 8, 24)    96          ['conv2d_117[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," conv2d_118 (Conv2D)            (None, 8, 8, 144)    3600        ['batch_normalization_179[0][0]']\n","                                                                                                  \n"," batch_normalization_180 (Batch  (None, 8, 8, 144)   576         ['conv2d_118[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_118 (ReLU)               (None, 8, 8, 144)    0           ['batch_normalization_180[0][0]']\n","                                                                                                  \n"," depthwise_conv2d_59 (Depthwise  (None, 8, 8, 144)   1440        ['re_lu_118[0][0]']              \n"," Conv2D)                                                                                          \n","                                                                                                  \n"," batch_normalization_181 (Batch  (None, 8, 8, 144)   576         ['depthwise_conv2d_59[0][0]']    \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_119 (ReLU)               (None, 8, 8, 144)    0           ['batch_normalization_181[0][0]']\n","                                                                                                  \n"," conv2d_119 (Conv2D)            (None, 8, 8, 24)     3480        ['re_lu_119[0][0]']              \n","                                                                                                  \n"," batch_normalization_182 (Batch  (None, 8, 8, 24)    96          ['conv2d_119[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," Residual_2b (Add)              (None, 8, 8, 24)     0           ['batch_normalization_179[0][0]',\n","                                                                  'batch_normalization_182[0][0]']\n","                                                                                                  \n"," conv2d_120 (Conv2D)            (None, 8, 8, 144)    3600        ['Residual_2b[0][0]']            \n","                                                                                                  \n"," batch_normalization_183 (Batch  (None, 8, 8, 144)   576         ['conv2d_120[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_120 (ReLU)               (None, 8, 8, 144)    0           ['batch_normalization_183[0][0]']\n","                                                                                                  \n"," depthwise_conv2d_60 (Depthwise  (None, 4, 4, 144)   1440        ['re_lu_120[0][0]']              \n"," Conv2D)                                                                                          \n","                                                                                                  \n"," batch_normalization_184 (Batch  (None, 4, 4, 144)   576         ['depthwise_conv2d_60[0][0]']    \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_121 (ReLU)               (None, 4, 4, 144)    0           ['batch_normalization_184[0][0]']\n","                                                                                                  \n"," conv2d_121 (Conv2D)            (None, 4, 4, 32)     4640        ['re_lu_121[0][0]']              \n","                                                                                                  \n"," batch_normalization_185 (Batch  (None, 4, 4, 32)    128         ['conv2d_121[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," conv2d_122 (Conv2D)            (None, 4, 4, 192)    6336        ['batch_normalization_185[0][0]']\n","                                                                                                  \n"," batch_normalization_186 (Batch  (None, 4, 4, 192)   768         ['conv2d_122[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_122 (ReLU)               (None, 4, 4, 192)    0           ['batch_normalization_186[0][0]']\n","                                                                                                  \n"," depthwise_conv2d_61 (Depthwise  (None, 4, 4, 192)   1920        ['re_lu_122[0][0]']              \n"," Conv2D)                                                                                          \n","                                                                                                  \n"," batch_normalization_187 (Batch  (None, 4, 4, 192)   768         ['depthwise_conv2d_61[0][0]']    \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_123 (ReLU)               (None, 4, 4, 192)    0           ['batch_normalization_187[0][0]']\n","                                                                                                  \n"," conv2d_123 (Conv2D)            (None, 4, 4, 32)     6176        ['re_lu_123[0][0]']              \n","                                                                                                  \n"," batch_normalization_188 (Batch  (None, 4, 4, 32)    128         ['conv2d_123[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," Residual_3b (Add)              (None, 4, 4, 32)     0           ['batch_normalization_185[0][0]',\n","                                                                  'batch_normalization_188[0][0]']\n","                                                                                                  \n"," conv2d_124 (Conv2D)            (None, 4, 4, 192)    6336        ['Residual_3b[0][0]']            \n","                                                                                                  \n"," batch_normalization_189 (Batch  (None, 4, 4, 192)   768         ['conv2d_124[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_124 (ReLU)               (None, 4, 4, 192)    0           ['batch_normalization_189[0][0]']\n","                                                                                                  \n"," depthwise_conv2d_62 (Depthwise  (None, 4, 4, 192)   1920        ['re_lu_124[0][0]']              \n"," Conv2D)                                                                                          \n","                                                                                                  \n"," batch_normalization_190 (Batch  (None, 4, 4, 192)   768         ['depthwise_conv2d_62[0][0]']    \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_125 (ReLU)               (None, 4, 4, 192)    0           ['batch_normalization_190[0][0]']\n","                                                                                                  \n"," conv2d_125 (Conv2D)            (None, 4, 4, 32)     6176        ['re_lu_125[0][0]']              \n","                                                                                                  \n"," batch_normalization_191 (Batch  (None, 4, 4, 32)    128         ['conv2d_125[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," Residual_3c (Add)              (None, 4, 4, 32)     0           ['Residual_3b[0][0]',            \n","                                                                  'batch_normalization_191[0][0]']\n","                                                                                                  \n"," conv2d_126 (Conv2D)            (None, 4, 4, 192)    6336        ['Residual_3c[0][0]']            \n","                                                                                                  \n"," batch_normalization_192 (Batch  (None, 4, 4, 192)   768         ['conv2d_126[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_126 (ReLU)               (None, 4, 4, 192)    0           ['batch_normalization_192[0][0]']\n","                                                                                                  \n"," depthwise_conv2d_63 (Depthwise  (None, 2, 2, 192)   1920        ['re_lu_126[0][0]']              \n"," Conv2D)                                                                                          \n","                                                                                                  \n"," batch_normalization_193 (Batch  (None, 2, 2, 192)   768         ['depthwise_conv2d_63[0][0]']    \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_127 (ReLU)               (None, 2, 2, 192)    0           ['batch_normalization_193[0][0]']\n","                                                                                                  \n"," conv2d_127 (Conv2D)            (None, 2, 2, 64)     12352       ['re_lu_127[0][0]']              \n","                                                                                                  \n"," batch_normalization_194 (Batch  (None, 2, 2, 64)    256         ['conv2d_127[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," conv2d_128 (Conv2D)            (None, 2, 2, 384)    24960       ['batch_normalization_194[0][0]']\n","                                                                                                  \n"," batch_normalization_195 (Batch  (None, 2, 2, 384)   1536        ['conv2d_128[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_128 (ReLU)               (None, 2, 2, 384)    0           ['batch_normalization_195[0][0]']\n","                                                                                                  \n"," depthwise_conv2d_64 (Depthwise  (None, 2, 2, 384)   3840        ['re_lu_128[0][0]']              \n"," Conv2D)                                                                                          \n","                                                                                                  \n"," batch_normalization_196 (Batch  (None, 2, 2, 384)   1536        ['depthwise_conv2d_64[0][0]']    \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_129 (ReLU)               (None, 2, 2, 384)    0           ['batch_normalization_196[0][0]']\n","                                                                                                  \n"," conv2d_129 (Conv2D)            (None, 2, 2, 64)     24640       ['re_lu_129[0][0]']              \n","                                                                                                  \n"," batch_normalization_197 (Batch  (None, 2, 2, 64)    256         ['conv2d_129[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," Residual_4b (Add)              (None, 2, 2, 64)     0           ['batch_normalization_194[0][0]',\n","                                                                  'batch_normalization_197[0][0]']\n","                                                                                                  \n"," conv2d_130 (Conv2D)            (None, 2, 2, 384)    24960       ['Residual_4b[0][0]']            \n","                                                                                                  \n"," batch_normalization_198 (Batch  (None, 2, 2, 384)   1536        ['conv2d_130[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_130 (ReLU)               (None, 2, 2, 384)    0           ['batch_normalization_198[0][0]']\n","                                                                                                  \n"," depthwise_conv2d_65 (Depthwise  (None, 2, 2, 384)   3840        ['re_lu_130[0][0]']              \n"," Conv2D)                                                                                          \n","                                                                                                  \n"," batch_normalization_199 (Batch  (None, 2, 2, 384)   1536        ['depthwise_conv2d_65[0][0]']    \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_131 (ReLU)               (None, 2, 2, 384)    0           ['batch_normalization_199[0][0]']\n","                                                                                                  \n"," conv2d_131 (Conv2D)            (None, 2, 2, 64)     24640       ['re_lu_131[0][0]']              \n","                                                                                                  \n"," batch_normalization_200 (Batch  (None, 2, 2, 64)    256         ['conv2d_131[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," Residual_4c (Add)              (None, 2, 2, 64)     0           ['Residual_4b[0][0]',            \n","                                                                  'batch_normalization_200[0][0]']\n","                                                                                                  \n"," conv2d_132 (Conv2D)            (None, 2, 2, 384)    24960       ['Residual_4c[0][0]']            \n","                                                                                                  \n"," batch_normalization_201 (Batch  (None, 2, 2, 384)   1536        ['conv2d_132[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_132 (ReLU)               (None, 2, 2, 384)    0           ['batch_normalization_201[0][0]']\n","                                                                                                  \n"," depthwise_conv2d_66 (Depthwise  (None, 2, 2, 384)   3840        ['re_lu_132[0][0]']              \n"," Conv2D)                                                                                          \n","                                                                                                  \n"," batch_normalization_202 (Batch  (None, 2, 2, 384)   1536        ['depthwise_conv2d_66[0][0]']    \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_133 (ReLU)               (None, 2, 2, 384)    0           ['batch_normalization_202[0][0]']\n","                                                                                                  \n"," conv2d_133 (Conv2D)            (None, 2, 2, 64)     24640       ['re_lu_133[0][0]']              \n","                                                                                                  \n"," batch_normalization_203 (Batch  (None, 2, 2, 64)    256         ['conv2d_133[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," Residual_4d (Add)              (None, 2, 2, 64)     0           ['Residual_4c[0][0]',            \n","                                                                  'batch_normalization_203[0][0]']\n","                                                                                                  \n"," dropout_12 (Dropout)           (None, 2, 2, 64)     0           ['Residual_4d[0][0]']            \n","                                                                                                  \n"," conv2d_134 (Conv2D)            (None, 2, 2, 384)    24960       ['dropout_12[0][0]']             \n","                                                                                                  \n"," batch_normalization_204 (Batch  (None, 2, 2, 384)   1536        ['conv2d_134[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_134 (ReLU)               (None, 2, 2, 384)    0           ['batch_normalization_204[0][0]']\n","                                                                                                  \n"," depthwise_conv2d_67 (Depthwise  (None, 2, 2, 384)   3840        ['re_lu_134[0][0]']              \n"," Conv2D)                                                                                          \n","                                                                                                  \n"," batch_normalization_205 (Batch  (None, 2, 2, 384)   1536        ['depthwise_conv2d_67[0][0]']    \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_135 (ReLU)               (None, 2, 2, 384)    0           ['batch_normalization_205[0][0]']\n","                                                                                                  \n"," conv2d_135 (Conv2D)            (None, 2, 2, 96)     36960       ['re_lu_135[0][0]']              \n","                                                                                                  \n"," batch_normalization_206 (Batch  (None, 2, 2, 96)    384         ['conv2d_135[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," conv2d_136 (Conv2D)            (None, 2, 2, 576)    55872       ['batch_normalization_206[0][0]']\n","                                                                                                  \n"," batch_normalization_207 (Batch  (None, 2, 2, 576)   2304        ['conv2d_136[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_136 (ReLU)               (None, 2, 2, 576)    0           ['batch_normalization_207[0][0]']\n","                                                                                                  \n"," depthwise_conv2d_68 (Depthwise  (None, 2, 2, 576)   5760        ['re_lu_136[0][0]']              \n"," Conv2D)                                                                                          \n","                                                                                                  \n"," batch_normalization_208 (Batch  (None, 2, 2, 576)   2304        ['depthwise_conv2d_68[0][0]']    \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_137 (ReLU)               (None, 2, 2, 576)    0           ['batch_normalization_208[0][0]']\n","                                                                                                  \n"," conv2d_137 (Conv2D)            (None, 2, 2, 96)     55392       ['re_lu_137[0][0]']              \n","                                                                                                  \n"," batch_normalization_209 (Batch  (None, 2, 2, 96)    384         ['conv2d_137[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," Residual_5b (Add)              (None, 2, 2, 96)     0           ['batch_normalization_206[0][0]',\n","                                                                  'batch_normalization_209[0][0]']\n","                                                                                                  \n"," conv2d_150 (Conv2D)            (None, 2, 2, 576)    55872       ['Residual_5b[0][0]']            \n","                                                                                                  \n"," batch_normalization_228 (Batch  (None, 2, 2, 576)   2304        ['conv2d_150[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_150 (ReLU)               (None, 2, 2, 576)    0           ['batch_normalization_228[0][0]']\n","                                                                                                  \n"," depthwise_conv2d_75 (Depthwise  (None, 2, 2, 576)   5760        ['re_lu_150[0][0]']              \n"," Conv2D)                                                                                          \n","                                                                                                  \n"," batch_normalization_229 (Batch  (None, 2, 2, 576)   2304        ['depthwise_conv2d_75[0][0]']    \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_151 (ReLU)               (None, 2, 2, 576)    0           ['batch_normalization_229[0][0]']\n","                                                                                                  \n"," conv2d_151 (Conv2D)            (None, 2, 2, 320)    184640      ['re_lu_151[0][0]']              \n","                                                                                                  \n"," batch_normalization_230 (Batch  (None, 2, 2, 320)   1280        ['conv2d_151[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," dropout_15 (Dropout)           (None, 2, 2, 320)    0           ['batch_normalization_230[0][0]']\n","                                                                                                  \n"," final_conv2d_3 (final_conv2d)  (None, 2, 2, 1280)   416000      ['dropout_15[0][0]']             \n","                                                                                                  \n"," flatten_3 (Flatten)            (None, 5120)         0           ['final_conv2d_3[0][0]']         \n","                                                                                                  \n"," dense_3 (Dense)                (None, 10)           51210       ['flatten_3[0][0]']              \n","                                                                                                  \n"," batch_normalization_231 (Batch  (None, 10)          40          ['dense_3[0][0]']                \n"," Normalization)                                                                                   \n","                                                                                                  \n"," softmax_3 (Softmax)            (None, 10)           0           ['batch_normalization_231[0][0]']\n","                                                                                                  \n","==================================================================================================\n","Total params: 1,165,298\n","Trainable params: 1,146,078\n","Non-trainable params: 19,220\n","__________________________________________________________________________________________________\n","Epoch 1/300\n","521/521 [==============================] - ETA: 0s - loss: 1.7363 - accuracy: 0.3400\n","Epoch 00001: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 45s 75ms/step - loss: 1.7363 - accuracy: 0.3400 - val_loss: 28.0549 - val_accuracy: 0.1000\n","Epoch 2/300\n","521/521 [==============================] - ETA: 0s - loss: 1.4077 - accuracy: 0.4860\n","Epoch 00002: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 74ms/step - loss: 1.4077 - accuracy: 0.4860 - val_loss: 19.8068 - val_accuracy: 0.1000\n","Epoch 3/300\n","521/521 [==============================] - ETA: 0s - loss: 1.2657 - accuracy: 0.5467\n","Epoch 00003: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 1.2657 - accuracy: 0.5467 - val_loss: 14.1456 - val_accuracy: 0.1513\n","Epoch 4/300\n","521/521 [==============================] - ETA: 0s - loss: 1.1912 - accuracy: 0.5743\n","Epoch 00004: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 72ms/step - loss: 1.1912 - accuracy: 0.5743 - val_loss: 15.0662 - val_accuracy: 0.1015\n","Epoch 5/300\n","521/521 [==============================] - ETA: 0s - loss: 1.1707 - accuracy: 0.5794\n","Epoch 00005: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 72ms/step - loss: 1.1707 - accuracy: 0.5794 - val_loss: 14.3694 - val_accuracy: 0.1000\n","Epoch 6/300\n","521/521 [==============================] - ETA: 0s - loss: 1.1593 - accuracy: 0.5833\n","Epoch 00006: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 72ms/step - loss: 1.1593 - accuracy: 0.5833 - val_loss: 12.2758 - val_accuracy: 0.1066\n","Epoch 7/300\n","521/521 [==============================] - ETA: 0s - loss: 1.1518 - accuracy: 0.5889\n","Epoch 00007: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 72ms/step - loss: 1.1518 - accuracy: 0.5889 - val_loss: 12.6210 - val_accuracy: 0.1036\n","Epoch 8/300\n","521/521 [==============================] - ETA: 0s - loss: 1.1506 - accuracy: 0.5891\n","Epoch 00008: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 1.1506 - accuracy: 0.5891 - val_loss: 10.3852 - val_accuracy: 0.1243\n","Epoch 9/300\n","521/521 [==============================] - ETA: 0s - loss: 1.1586 - accuracy: 0.5840\n","Epoch 00009: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 1.1586 - accuracy: 0.5840 - val_loss: 10.4081 - val_accuracy: 0.1085\n","Epoch 10/300\n","521/521 [==============================] - ETA: 0s - loss: 1.1864 - accuracy: 0.5743\n","Epoch 00010: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 1.1864 - accuracy: 0.5743 - val_loss: 8.2918 - val_accuracy: 0.1299\n","Epoch 11/300\n","521/521 [==============================] - ETA: 0s - loss: 1.2057 - accuracy: 0.5676\n","Epoch 00011: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 1.2057 - accuracy: 0.5676 - val_loss: 7.5481 - val_accuracy: 0.1309\n","Epoch 12/300\n","521/521 [==============================] - ETA: 0s - loss: 1.2278 - accuracy: 0.5588\n","Epoch 00012: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 1.2278 - accuracy: 0.5588 - val_loss: 4.3169 - val_accuracy: 0.2302\n","Epoch 13/300\n","521/521 [==============================] - ETA: 0s - loss: 1.1946 - accuracy: 0.5700\n","Epoch 00013: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 1.1946 - accuracy: 0.5700 - val_loss: 7.9764 - val_accuracy: 0.1002\n","Epoch 14/300\n","521/521 [==============================] - ETA: 0s - loss: 1.2491 - accuracy: 0.5507\n","Epoch 00014: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 1.2491 - accuracy: 0.5507 - val_loss: 4.3538 - val_accuracy: 0.2230\n","Epoch 15/300\n","521/521 [==============================] - ETA: 0s - loss: 1.2930 - accuracy: 0.5357\n","Epoch 00015: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 1.2930 - accuracy: 0.5357 - val_loss: 6.6658 - val_accuracy: 0.1233\n","Epoch 16/300\n","521/521 [==============================] - ETA: 0s - loss: 1.2544 - accuracy: 0.5488\n","Epoch 00016: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 1.2544 - accuracy: 0.5488 - val_loss: 4.0531 - val_accuracy: 0.1950\n","Epoch 17/300\n","521/521 [==============================] - ETA: 0s - loss: 1.1996 - accuracy: 0.5740\n","Epoch 00017: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 72ms/step - loss: 1.1996 - accuracy: 0.5740 - val_loss: 4.4370 - val_accuracy: 0.1772\n","Epoch 18/300\n","521/521 [==============================] - ETA: 0s - loss: 1.1722 - accuracy: 0.5832\n","Epoch 00018: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 1.1722 - accuracy: 0.5832 - val_loss: 3.7301 - val_accuracy: 0.2506\n","Epoch 19/300\n","521/521 [==============================] - ETA: 0s - loss: 1.1484 - accuracy: 0.5881\n","Epoch 00019: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 1.1484 - accuracy: 0.5881 - val_loss: 3.3524 - val_accuracy: 0.3001\n","Epoch 20/300\n","521/521 [==============================] - ETA: 0s - loss: 1.1357 - accuracy: 0.5939\n","Epoch 00020: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 72ms/step - loss: 1.1357 - accuracy: 0.5939 - val_loss: 2.9762 - val_accuracy: 0.2168\n","Epoch 21/300\n","521/521 [==============================] - ETA: 0s - loss: 1.1228 - accuracy: 0.5967\n","Epoch 00021: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 74ms/step - loss: 1.1228 - accuracy: 0.5967 - val_loss: 2.3446 - val_accuracy: 0.3463\n","Epoch 22/300\n","521/521 [==============================] - ETA: 0s - loss: 1.1570 - accuracy: 0.5854\n","Epoch 00022: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 72ms/step - loss: 1.1570 - accuracy: 0.5854 - val_loss: 4.7778 - val_accuracy: 0.2375\n","Epoch 23/300\n","521/521 [==============================] - ETA: 0s - loss: 1.1128 - accuracy: 0.6019\n","Epoch 00023: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 1.1128 - accuracy: 0.6019 - val_loss: 3.8343 - val_accuracy: 0.1787\n","Epoch 24/300\n","521/521 [==============================] - ETA: 0s - loss: 1.0974 - accuracy: 0.6102\n","Epoch 00024: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 1.0974 - accuracy: 0.6102 - val_loss: 4.4101 - val_accuracy: 0.1203\n","Epoch 25/300\n","521/521 [==============================] - ETA: 0s - loss: 1.0763 - accuracy: 0.6155\n","Epoch 00025: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 1.0763 - accuracy: 0.6155 - val_loss: 4.1897 - val_accuracy: 0.2712\n","Epoch 26/300\n","521/521 [==============================] - ETA: 0s - loss: 1.0663 - accuracy: 0.6217\n","Epoch 00026: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 72ms/step - loss: 1.0663 - accuracy: 0.6217 - val_loss: 2.3106 - val_accuracy: 0.2989\n","Epoch 27/300\n","521/521 [==============================] - ETA: 0s - loss: 1.0533 - accuracy: 0.6277\n","Epoch 00027: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 72ms/step - loss: 1.0533 - accuracy: 0.6277 - val_loss: 1.7140 - val_accuracy: 0.4665\n","Epoch 28/300\n","521/521 [==============================] - ETA: 0s - loss: 1.0466 - accuracy: 0.6265\n","Epoch 00028: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 74ms/step - loss: 1.0466 - accuracy: 0.6265 - val_loss: 2.1913 - val_accuracy: 0.3488\n","Epoch 29/300\n","521/521 [==============================] - ETA: 0s - loss: 1.0383 - accuracy: 0.6295\n","Epoch 00029: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 74ms/step - loss: 1.0383 - accuracy: 0.6295 - val_loss: 2.3704 - val_accuracy: 0.3476\n","Epoch 30/300\n","521/521 [==============================] - ETA: 0s - loss: 1.0301 - accuracy: 0.6326\n","Epoch 00030: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 1.0301 - accuracy: 0.6326 - val_loss: 2.7860 - val_accuracy: 0.3636\n","Epoch 31/300\n","521/521 [==============================] - ETA: 0s - loss: 1.0133 - accuracy: 0.6375\n","Epoch 00031: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 1.0133 - accuracy: 0.6375 - val_loss: 2.6908 - val_accuracy: 0.2771\n","Epoch 32/300\n","521/521 [==============================] - ETA: 0s - loss: 1.0066 - accuracy: 0.6405\n","Epoch 00032: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 1.0066 - accuracy: 0.6405 - val_loss: 3.2188 - val_accuracy: 0.2848\n","Epoch 33/300\n","521/521 [==============================] - ETA: 0s - loss: 0.9979 - accuracy: 0.6436\n","Epoch 00033: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 72ms/step - loss: 0.9979 - accuracy: 0.6436 - val_loss: 3.0504 - val_accuracy: 0.1990\n","Epoch 34/300\n","521/521 [==============================] - ETA: 0s - loss: 0.9865 - accuracy: 0.6478\n","Epoch 00034: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.9865 - accuracy: 0.6478 - val_loss: 2.7182 - val_accuracy: 0.2918\n","Epoch 35/300\n","521/521 [==============================] - ETA: 0s - loss: 0.9761 - accuracy: 0.6517\n","Epoch 00035: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 72ms/step - loss: 0.9761 - accuracy: 0.6517 - val_loss: 2.7357 - val_accuracy: 0.3280\n","Epoch 36/300\n","521/521 [==============================] - ETA: 0s - loss: 0.9664 - accuracy: 0.6540\n","Epoch 00036: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.9664 - accuracy: 0.6540 - val_loss: 2.1005 - val_accuracy: 0.4252\n","Epoch 37/300\n","521/521 [==============================] - ETA: 0s - loss: 0.9545 - accuracy: 0.6597\n","Epoch 00037: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 72ms/step - loss: 0.9545 - accuracy: 0.6597 - val_loss: 2.0957 - val_accuracy: 0.4029\n","Epoch 38/300\n","521/521 [==============================] - ETA: 0s - loss: 0.9456 - accuracy: 0.6636\n","Epoch 00038: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 72ms/step - loss: 0.9456 - accuracy: 0.6636 - val_loss: 2.3325 - val_accuracy: 0.3613\n","Epoch 39/300\n","521/521 [==============================] - ETA: 0s - loss: 0.9374 - accuracy: 0.6669\n","Epoch 00039: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.9374 - accuracy: 0.6669 - val_loss: 2.3114 - val_accuracy: 0.3854\n","Epoch 40/300\n","521/521 [==============================] - ETA: 0s - loss: 0.9304 - accuracy: 0.6666\n","Epoch 00040: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.9304 - accuracy: 0.6666 - val_loss: 1.6757 - val_accuracy: 0.4975\n","Epoch 41/300\n","521/521 [==============================] - ETA: 0s - loss: 0.9274 - accuracy: 0.6700\n","Epoch 00041: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.9274 - accuracy: 0.6700 - val_loss: 2.0322 - val_accuracy: 0.3701\n","Epoch 42/300\n","521/521 [==============================] - ETA: 0s - loss: 0.9263 - accuracy: 0.6695\n","Epoch 00042: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.9263 - accuracy: 0.6695 - val_loss: 1.3646 - val_accuracy: 0.5633\n","Epoch 43/300\n","521/521 [==============================] - ETA: 0s - loss: 0.9163 - accuracy: 0.6738\n","Epoch 00043: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.9163 - accuracy: 0.6738 - val_loss: 1.2752 - val_accuracy: 0.5567\n","Epoch 44/300\n","521/521 [==============================] - ETA: 0s - loss: 0.9098 - accuracy: 0.6763\n","Epoch 00044: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 74ms/step - loss: 0.9098 - accuracy: 0.6763 - val_loss: 2.2705 - val_accuracy: 0.3597\n","Epoch 45/300\n","521/521 [==============================] - ETA: 0s - loss: 0.9000 - accuracy: 0.6787\n","Epoch 00045: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.9000 - accuracy: 0.6787 - val_loss: 3.6317 - val_accuracy: 0.3228\n","Epoch 46/300\n","521/521 [==============================] - ETA: 0s - loss: 0.8960 - accuracy: 0.6814\n","Epoch 00046: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.8960 - accuracy: 0.6814 - val_loss: 3.7088 - val_accuracy: 0.1953\n","Epoch 47/300\n","521/521 [==============================] - ETA: 0s - loss: 0.8839 - accuracy: 0.6833\n","Epoch 00047: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.8839 - accuracy: 0.6833 - val_loss: 1.2314 - val_accuracy: 0.5691\n","Epoch 48/300\n","521/521 [==============================] - ETA: 0s - loss: 0.8800 - accuracy: 0.6853\n","Epoch 00048: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.8800 - accuracy: 0.6853 - val_loss: 1.3923 - val_accuracy: 0.5389\n","Epoch 49/300\n","521/521 [==============================] - ETA: 0s - loss: 0.8701 - accuracy: 0.6893\n","Epoch 00049: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 72ms/step - loss: 0.8701 - accuracy: 0.6893 - val_loss: 2.4070 - val_accuracy: 0.3414\n","Epoch 50/300\n","521/521 [==============================] - ETA: 0s - loss: 0.8716 - accuracy: 0.6913\n","Epoch 00050: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 72ms/step - loss: 0.8716 - accuracy: 0.6913 - val_loss: 2.1565 - val_accuracy: 0.3974\n","Epoch 51/300\n","521/521 [==============================] - ETA: 0s - loss: 0.8626 - accuracy: 0.6916\n","Epoch 00051: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.8626 - accuracy: 0.6916 - val_loss: 1.8782 - val_accuracy: 0.4005\n","Epoch 52/300\n","521/521 [==============================] - ETA: 0s - loss: 0.8553 - accuracy: 0.6951\n","Epoch 00052: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 74ms/step - loss: 0.8553 - accuracy: 0.6951 - val_loss: 2.1821 - val_accuracy: 0.3197\n","Epoch 53/300\n","521/521 [==============================] - ETA: 0s - loss: 0.8525 - accuracy: 0.6950\n","Epoch 00053: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.8525 - accuracy: 0.6950 - val_loss: 2.1889 - val_accuracy: 0.3768\n","Epoch 54/300\n","521/521 [==============================] - ETA: 0s - loss: 0.8434 - accuracy: 0.7003\n","Epoch 00054: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.8434 - accuracy: 0.7003 - val_loss: 1.5935 - val_accuracy: 0.5152\n","Epoch 55/300\n","521/521 [==============================] - ETA: 0s - loss: 0.8373 - accuracy: 0.7016\n","Epoch 00055: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 72ms/step - loss: 0.8373 - accuracy: 0.7016 - val_loss: 1.5544 - val_accuracy: 0.5144\n","Epoch 56/300\n","521/521 [==============================] - ETA: 0s - loss: 0.8299 - accuracy: 0.7050\n","Epoch 00056: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.8299 - accuracy: 0.7050 - val_loss: 4.3420 - val_accuracy: 0.1661\n","Epoch 57/300\n","521/521 [==============================] - ETA: 0s - loss: 0.8238 - accuracy: 0.7057\n","Epoch 00057: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.8238 - accuracy: 0.7057 - val_loss: 1.5234 - val_accuracy: 0.5359\n","Epoch 58/300\n","521/521 [==============================] - ETA: 0s - loss: 0.8206 - accuracy: 0.7073\n","Epoch 00058: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.8206 - accuracy: 0.7073 - val_loss: 1.9825 - val_accuracy: 0.4007\n","Epoch 59/300\n","521/521 [==============================] - ETA: 0s - loss: 0.8123 - accuracy: 0.7103\n","Epoch 00059: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 74ms/step - loss: 0.8123 - accuracy: 0.7103 - val_loss: 1.6923 - val_accuracy: 0.4723\n","Epoch 60/300\n","521/521 [==============================] - ETA: 0s - loss: 0.8033 - accuracy: 0.7144\n","Epoch 00060: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.8033 - accuracy: 0.7144 - val_loss: 2.8500 - val_accuracy: 0.2991\n","Epoch 61/300\n","521/521 [==============================] - ETA: 0s - loss: 0.8058 - accuracy: 0.7126\n","Epoch 00061: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.8058 - accuracy: 0.7126 - val_loss: 3.2509 - val_accuracy: 0.2974\n","Epoch 62/300\n","521/521 [==============================] - ETA: 0s - loss: 0.7990 - accuracy: 0.7137\n","Epoch 00062: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.7990 - accuracy: 0.7137 - val_loss: 3.7669 - val_accuracy: 0.2437\n","Epoch 63/300\n","521/521 [==============================] - ETA: 0s - loss: 0.7950 - accuracy: 0.7167\n","Epoch 00063: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.7950 - accuracy: 0.7167 - val_loss: 3.9355 - val_accuracy: 0.2193\n","Epoch 64/300\n","521/521 [==============================] - ETA: 0s - loss: 0.7903 - accuracy: 0.7180\n","Epoch 00064: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.7903 - accuracy: 0.7180 - val_loss: 1.5500 - val_accuracy: 0.5038\n","Epoch 65/300\n","521/521 [==============================] - ETA: 0s - loss: 0.7825 - accuracy: 0.7208\n","Epoch 00065: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 74ms/step - loss: 0.7825 - accuracy: 0.7208 - val_loss: 1.9693 - val_accuracy: 0.4457\n","Epoch 66/300\n","521/521 [==============================] - ETA: 0s - loss: 0.7747 - accuracy: 0.7242\n","Epoch 00066: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.7747 - accuracy: 0.7242 - val_loss: 3.3615 - val_accuracy: 0.2501\n","Epoch 67/300\n","521/521 [==============================] - ETA: 0s - loss: 0.7723 - accuracy: 0.7220\n","Epoch 00067: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 74ms/step - loss: 0.7723 - accuracy: 0.7220 - val_loss: 1.5518 - val_accuracy: 0.4915\n","Epoch 68/300\n","521/521 [==============================] - ETA: 0s - loss: 0.7691 - accuracy: 0.7237\n","Epoch 00068: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.7691 - accuracy: 0.7237 - val_loss: 2.8318 - val_accuracy: 0.3515\n","Epoch 69/300\n","521/521 [==============================] - ETA: 0s - loss: 0.7563 - accuracy: 0.7283\n","Epoch 00069: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.7563 - accuracy: 0.7283 - val_loss: 2.7678 - val_accuracy: 0.3386\n","Epoch 70/300\n","521/521 [==============================] - ETA: 0s - loss: 0.7543 - accuracy: 0.7290\n","Epoch 00070: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.7543 - accuracy: 0.7290 - val_loss: 2.9626 - val_accuracy: 0.2885\n","Epoch 71/300\n","521/521 [==============================] - ETA: 0s - loss: 0.7494 - accuracy: 0.7317\n","Epoch 00071: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 74ms/step - loss: 0.7494 - accuracy: 0.7317 - val_loss: 5.6807 - val_accuracy: 0.2050\n","Epoch 72/300\n","521/521 [==============================] - ETA: 0s - loss: 0.7428 - accuracy: 0.7352\n","Epoch 00072: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.7428 - accuracy: 0.7352 - val_loss: 3.6089 - val_accuracy: 0.2473\n","Epoch 73/300\n","521/521 [==============================] - ETA: 0s - loss: 0.7437 - accuracy: 0.7329\n","Epoch 00073: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.7437 - accuracy: 0.7329 - val_loss: 4.4318 - val_accuracy: 0.1913\n","Epoch 74/300\n","521/521 [==============================] - ETA: 0s - loss: 0.7308 - accuracy: 0.7383\n","Epoch 00074: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.7308 - accuracy: 0.7383 - val_loss: 2.9705 - val_accuracy: 0.3656\n","Epoch 75/300\n","521/521 [==============================] - ETA: 0s - loss: 0.7261 - accuracy: 0.7400\n","Epoch 00075: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.7261 - accuracy: 0.7400 - val_loss: 3.5016 - val_accuracy: 0.2842\n","Epoch 76/300\n","521/521 [==============================] - ETA: 0s - loss: 0.7248 - accuracy: 0.7412\n","Epoch 00076: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.7248 - accuracy: 0.7412 - val_loss: 3.7920 - val_accuracy: 0.2348\n","Epoch 77/300\n","521/521 [==============================] - ETA: 0s - loss: 0.7174 - accuracy: 0.7435\n","Epoch 00077: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.7174 - accuracy: 0.7435 - val_loss: 4.4916 - val_accuracy: 0.2152\n","Epoch 78/300\n","521/521 [==============================] - ETA: 0s - loss: 0.7130 - accuracy: 0.7450\n","Epoch 00078: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 74ms/step - loss: 0.7130 - accuracy: 0.7450 - val_loss: 1.6408 - val_accuracy: 0.5071\n","Epoch 79/300\n","521/521 [==============================] - ETA: 0s - loss: 0.7090 - accuracy: 0.7464\n","Epoch 00079: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 74ms/step - loss: 0.7090 - accuracy: 0.7464 - val_loss: 2.3495 - val_accuracy: 0.3983\n","Epoch 80/300\n","521/521 [==============================] - ETA: 0s - loss: 0.7048 - accuracy: 0.7478\n","Epoch 00080: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.7048 - accuracy: 0.7478 - val_loss: 2.6679 - val_accuracy: 0.3686\n","Epoch 81/300\n","521/521 [==============================] - ETA: 0s - loss: 0.6998 - accuracy: 0.7485\n","Epoch 00081: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.6998 - accuracy: 0.7485 - val_loss: 3.9250 - val_accuracy: 0.3089\n","Epoch 82/300\n","521/521 [==============================] - ETA: 0s - loss: 0.6988 - accuracy: 0.7492\n","Epoch 00082: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.6988 - accuracy: 0.7492 - val_loss: 2.6151 - val_accuracy: 0.4321\n","Epoch 83/300\n","521/521 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.7497\n","Epoch 00083: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 74ms/step - loss: 0.6933 - accuracy: 0.7497 - val_loss: 1.8567 - val_accuracy: 0.4597\n","Epoch 84/300\n","521/521 [==============================] - ETA: 0s - loss: 0.6859 - accuracy: 0.7510\n","Epoch 00084: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.6859 - accuracy: 0.7510 - val_loss: 1.1432 - val_accuracy: 0.6128\n","Epoch 85/300\n","521/521 [==============================] - ETA: 0s - loss: 0.6804 - accuracy: 0.7558\n","Epoch 00085: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.6804 - accuracy: 0.7558 - val_loss: 3.8082 - val_accuracy: 0.2569\n","Epoch 86/300\n","521/521 [==============================] - ETA: 0s - loss: 0.6770 - accuracy: 0.7553\n","Epoch 00086: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 74ms/step - loss: 0.6770 - accuracy: 0.7553 - val_loss: 1.7213 - val_accuracy: 0.4912\n","Epoch 87/300\n","521/521 [==============================] - ETA: 0s - loss: 0.6665 - accuracy: 0.7603\n","Epoch 00087: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.6665 - accuracy: 0.7603 - val_loss: 2.3716 - val_accuracy: 0.4394\n","Epoch 88/300\n","521/521 [==============================] - ETA: 0s - loss: 0.6654 - accuracy: 0.7601\n","Epoch 00088: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.6654 - accuracy: 0.7601 - val_loss: 2.7907 - val_accuracy: 0.3758\n","Epoch 89/300\n","521/521 [==============================] - ETA: 0s - loss: 0.6598 - accuracy: 0.7642\n","Epoch 00089: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 74ms/step - loss: 0.6598 - accuracy: 0.7642 - val_loss: 1.0676 - val_accuracy: 0.6333\n","Epoch 90/300\n","521/521 [==============================] - ETA: 0s - loss: 0.6592 - accuracy: 0.7619\n","Epoch 00090: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.6592 - accuracy: 0.7619 - val_loss: 2.4398 - val_accuracy: 0.4285\n","Epoch 91/300\n","521/521 [==============================] - ETA: 0s - loss: 0.6518 - accuracy: 0.7656\n","Epoch 00091: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.6518 - accuracy: 0.7656 - val_loss: 3.3359 - val_accuracy: 0.2728\n","Epoch 92/300\n","521/521 [==============================] - ETA: 0s - loss: 0.6514 - accuracy: 0.7670\n","Epoch 00092: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.6514 - accuracy: 0.7670 - val_loss: 1.7090 - val_accuracy: 0.5162\n","Epoch 93/300\n","521/521 [==============================] - ETA: 0s - loss: 0.6456 - accuracy: 0.7673\n","Epoch 00093: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.6456 - accuracy: 0.7673 - val_loss: 1.5929 - val_accuracy: 0.5518\n","Epoch 94/300\n","521/521 [==============================] - ETA: 0s - loss: 0.6406 - accuracy: 0.7688\n","Epoch 00094: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.6406 - accuracy: 0.7688 - val_loss: 0.9598 - val_accuracy: 0.6694\n","Epoch 95/300\n","521/521 [==============================] - ETA: 0s - loss: 0.6352 - accuracy: 0.7718\n","Epoch 00095: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.6352 - accuracy: 0.7718 - val_loss: 1.6065 - val_accuracy: 0.5515\n","Epoch 96/300\n","521/521 [==============================] - ETA: 0s - loss: 0.6325 - accuracy: 0.7736\n","Epoch 00096: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 74ms/step - loss: 0.6325 - accuracy: 0.7736 - val_loss: 1.1634 - val_accuracy: 0.6289\n","Epoch 97/300\n","521/521 [==============================] - ETA: 0s - loss: 0.6276 - accuracy: 0.7747\n","Epoch 00097: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 74ms/step - loss: 0.6276 - accuracy: 0.7747 - val_loss: 1.3286 - val_accuracy: 0.5899\n","Epoch 98/300\n","521/521 [==============================] - ETA: 0s - loss: 0.6249 - accuracy: 0.7744\n","Epoch 00098: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 74ms/step - loss: 0.6249 - accuracy: 0.7744 - val_loss: 3.0390 - val_accuracy: 0.3346\n","Epoch 99/300\n","521/521 [==============================] - ETA: 0s - loss: 0.6208 - accuracy: 0.7763\n","Epoch 00099: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 74ms/step - loss: 0.6208 - accuracy: 0.7763 - val_loss: 3.1749 - val_accuracy: 0.3278\n","Epoch 100/300\n","521/521 [==============================] - ETA: 0s - loss: 0.6181 - accuracy: 0.7775\n","Epoch 00100: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 74ms/step - loss: 0.6181 - accuracy: 0.7775 - val_loss: 2.3062 - val_accuracy: 0.3880\n","Epoch 101/300\n","521/521 [==============================] - ETA: 0s - loss: 0.6064 - accuracy: 0.7806\n","Epoch 00101: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 74ms/step - loss: 0.6064 - accuracy: 0.7806 - val_loss: 1.1512 - val_accuracy: 0.6179\n","Epoch 102/300\n","521/521 [==============================] - ETA: 0s - loss: 0.6077 - accuracy: 0.7806\n","Epoch 00102: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 74ms/step - loss: 0.6077 - accuracy: 0.7806 - val_loss: 1.5659 - val_accuracy: 0.5736\n","Epoch 103/300\n","521/521 [==============================] - ETA: 0s - loss: 0.6046 - accuracy: 0.7813\n","Epoch 00103: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.6046 - accuracy: 0.7813 - val_loss: 1.3266 - val_accuracy: 0.5903\n","Epoch 104/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5990 - accuracy: 0.7837\n","Epoch 00104: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 74ms/step - loss: 0.5990 - accuracy: 0.7837 - val_loss: 1.7710 - val_accuracy: 0.5119\n","Epoch 105/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5985 - accuracy: 0.7841\n","Epoch 00105: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.5985 - accuracy: 0.7841 - val_loss: 2.3095 - val_accuracy: 0.4475\n","Epoch 106/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5971 - accuracy: 0.7846\n","Epoch 00106: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 75ms/step - loss: 0.5971 - accuracy: 0.7846 - val_loss: 2.1875 - val_accuracy: 0.4531\n","Epoch 107/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5906 - accuracy: 0.7855\n","Epoch 00107: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 74ms/step - loss: 0.5906 - accuracy: 0.7855 - val_loss: 1.6664 - val_accuracy: 0.5289\n","Epoch 108/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5895 - accuracy: 0.7873\n","Epoch 00108: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 73ms/step - loss: 0.5895 - accuracy: 0.7873 - val_loss: 2.7380 - val_accuracy: 0.3756\n","Epoch 109/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5841 - accuracy: 0.7881\n","Epoch 00109: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 74ms/step - loss: 0.5841 - accuracy: 0.7881 - val_loss: 2.4644 - val_accuracy: 0.3888\n","Epoch 110/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5750 - accuracy: 0.7925\n","Epoch 00110: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 75ms/step - loss: 0.5750 - accuracy: 0.7925 - val_loss: 1.3535 - val_accuracy: 0.5983\n","Epoch 111/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5734 - accuracy: 0.7916\n","Epoch 00111: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 74ms/step - loss: 0.5734 - accuracy: 0.7916 - val_loss: 2.0186 - val_accuracy: 0.4916\n","Epoch 112/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5677 - accuracy: 0.7962\n","Epoch 00112: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 74ms/step - loss: 0.5677 - accuracy: 0.7962 - val_loss: 2.7757 - val_accuracy: 0.4330\n","Epoch 113/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5712 - accuracy: 0.7937\n","Epoch 00113: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 74ms/step - loss: 0.5712 - accuracy: 0.7937 - val_loss: 3.4345 - val_accuracy: 0.3131\n","Epoch 114/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5632 - accuracy: 0.7944\n","Epoch 00114: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 74ms/step - loss: 0.5632 - accuracy: 0.7944 - val_loss: 1.9390 - val_accuracy: 0.5258\n","Epoch 115/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5639 - accuracy: 0.7965\n","Epoch 00115: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 74ms/step - loss: 0.5639 - accuracy: 0.7965 - val_loss: 2.7233 - val_accuracy: 0.3877\n","Epoch 116/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5563 - accuracy: 0.7983\n","Epoch 00116: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 74ms/step - loss: 0.5563 - accuracy: 0.7983 - val_loss: 2.7990 - val_accuracy: 0.3667\n","Epoch 117/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5589 - accuracy: 0.7979\n","Epoch 00117: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 74ms/step - loss: 0.5589 - accuracy: 0.7979 - val_loss: 1.4765 - val_accuracy: 0.5746\n","Epoch 118/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5537 - accuracy: 0.7985\n","Epoch 00118: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 74ms/step - loss: 0.5537 - accuracy: 0.7985 - val_loss: 1.8661 - val_accuracy: 0.5095\n","Epoch 119/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5464 - accuracy: 0.8017\n","Epoch 00119: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 74ms/step - loss: 0.5464 - accuracy: 0.8017 - val_loss: 2.2574 - val_accuracy: 0.4648\n","Epoch 120/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5471 - accuracy: 0.8022\n","Epoch 00120: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 74ms/step - loss: 0.5471 - accuracy: 0.8022 - val_loss: 2.3283 - val_accuracy: 0.4376\n","Epoch 121/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5371 - accuracy: 0.8055\n","Epoch 00121: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 75ms/step - loss: 0.5371 - accuracy: 0.8055 - val_loss: 1.3758 - val_accuracy: 0.6015\n","Epoch 122/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5375 - accuracy: 0.8060\n","Epoch 00122: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 74ms/step - loss: 0.5375 - accuracy: 0.8060 - val_loss: 1.8538 - val_accuracy: 0.5047\n","Epoch 123/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5390 - accuracy: 0.8057\n","Epoch 00123: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 74ms/step - loss: 0.5390 - accuracy: 0.8057 - val_loss: 1.6362 - val_accuracy: 0.5760\n","Epoch 124/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5324 - accuracy: 0.8068\n","Epoch 00124: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 74ms/step - loss: 0.5324 - accuracy: 0.8068 - val_loss: 1.3548 - val_accuracy: 0.6107\n","Epoch 125/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5282 - accuracy: 0.8085\n","Epoch 00125: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 75ms/step - loss: 0.5282 - accuracy: 0.8085 - val_loss: 1.2144 - val_accuracy: 0.6413\n","Epoch 126/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5253 - accuracy: 0.8100\n","Epoch 00126: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 75ms/step - loss: 0.5253 - accuracy: 0.8100 - val_loss: 1.1068 - val_accuracy: 0.6624\n","Epoch 127/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5264 - accuracy: 0.8098\n","Epoch 00127: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 39s 74ms/step - loss: 0.5264 - accuracy: 0.8098 - val_loss: 1.3543 - val_accuracy: 0.6020\n","Epoch 128/300\n","521/521 [==============================] - ETA: 0s - loss: 0.5236 - accuracy: 0.8103\n","Epoch 00128: saving model to mobileNetV2/model.ckpt\n","521/521 [==============================] - 38s 74ms/step - loss: 0.5236 - accuracy: 0.8103 - val_loss: 1.0516 - val_accuracy: 0.6656\n","Epoch 129/300\n","155/521 [=======>......................] - ETA: 25s - loss: 0.5019 - accuracy: 0.8169"]}]},{"cell_type":"code","source":["!pip install matplotlib"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-FP-x0OnJfmb","executionInfo":{"status":"ok","timestamp":1639254089252,"user_tz":300,"elapsed":3356,"user":{"displayName":"Ryan Shue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg65rZPkz5bJ7ik0h6ITKz45vgUzj81potiaE77Dw=s64","userId":"11228421312565688614"}},"outputId":"3bb2a0d3-de11-4421-87c4-b6c60e01c862"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.6)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.imshow(test_x[2])\n","test_y[2]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"DrzWgjtjJy4g","executionInfo":{"status":"ok","timestamp":1639254400528,"user_tz":300,"elapsed":369,"user":{"displayName":"Ryan Shue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg65rZPkz5bJ7ik0h6ITKz45vgUzj81potiaE77Dw=s64","userId":"11228421312565688614"}},"outputId":"c877a12b-4878-439a-91bb-e59cbcb9b010"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8"]},"metadata":{},"execution_count":53},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["!zip -r /content/mobileNetV2.zip /content/mobileNetV2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FSx1wmbGLiwV","executionInfo":{"status":"ok","timestamp":1639288267235,"user_tz":300,"elapsed":995,"user":{"displayName":"Ryan Shue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg65rZPkz5bJ7ik0h6ITKz45vgUzj81potiaE77Dw=s64","userId":"11228421312565688614"}},"outputId":"e20c670e-23d7-475a-9dfd-3ed267321d85"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: content/mobileNetV2/ (stored 0%)\n","  adding: content/mobileNetV2/model.ckpt.index (deflated 80%)\n","  adding: content/mobileNetV2/model.ckpt.data-00000-of-00001 (deflated 39%)\n","  adding: content/mobileNetV2/checkpoint (deflated 42%)\n"]}]},{"cell_type":"code","source":["# Initial 32-output channel convolution\n","class initial_conv2d(tf.keras.Sequential):\n","  def __init__(self):\n","    tf.keras.Sequential.__init__(self, layers=[\n","      tf.keras.layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same', \n","                              activation=None, name='i_conv'),\n","      tf.keras.layers.BatchNormalization(momentum=0.999, name='i_bnorm'),\n","      tf.keras.layers.ReLU(max_value=6.0, name='i_relu')\n","      ])\n","    \n","# Final 1280-output channel convolution\n","class final_conv2d(tf.keras.Sequential):\n","  def __init__(self):\n","    tf.keras.Sequential.__init__(self, layers=[\n","      tf.keras.layers.Conv2D(1280, (1, 1),\n","                              activation=None, name='f_conv'),\n","      tf.keras.layers.BatchNormalization(momentum=0.999, name='f_bnorm'),\n","      tf.keras.layers.ReLU(max_value=6.0, name='f_relu')\n","      ])\n","    \n","# MBConv inverted residual bottleneck layer\n","def mbconv2d(input, expansion_factor=1, \n","                output_channels=16, stride=(1, 1), block_id=1):\n","    \n","    input_channels = input.shape[3]\n","\n","    # Initial 1x1 convolution, multiplying the channel count by the \n","    # expansion factor, widening the representation\n","    conv1 = tf.keras.layers.Conv2D(expansion_factor*input_channels, (1, 1), \n","                            activation=None)(input)\n","    conv1 = tf.keras.layers.BatchNormalization(momentum=0.999)(conv1)\n","    conv1 = tf.keras.layers.ReLU(max_value=6.0)(conv1)\n","    \n","    # Subsequent 3x3 Depthwise convolution, applying an independent kernel\n","    # For each input channel. Stride automatically performs downsampling.\n","    dwconv1 = tf.keras.layers.DepthwiseConv2D((3, 3), stride, padding='same', \n","                            activation='relu6')(conv1)\n","    dwconv1 = tf.keras.layers.BatchNormalization(momentum=0.999)(dwconv1)\n","    dwconv1 = tf.keras.layers.ReLU(max_value=6.0)(dwconv1)\n","    \n","    # Final 1x1 convolution, compressing the channel space into the output\n","    # Channel dimension. No nonlinear activation!\n","    conv2 = tf.keras.layers.Conv2D(output_channels, (1, 1),\n","                            activation=None)(dwconv1)\n","    conv2 = tf.keras.layers.BatchNormalization(momentum=0.999)(conv2)\n","\n","    # Residual connection\n","\n","    # As in ResNet, if the input and output dimensions to not line up, use\n","    # A linear transformation to map the identity into the output space\n","    # if input.shape != conv2.shape:\n","    #   input = tf.keras.layers.Conv2D(conv2.shape[3], (1, 1), \n","    #       activation=None, name='IdentityConv_' + block_id)(input)\n","    #   input = tf.keras.layers.MaxPooling2D(pool_size=(\n","    #       input.shape[1] / conv2.shape[1], input.shape[2] / \n","    #       conv2.shape[2]), name='IdentityMaxpool_' + block_id)(input)\n","    # return tf.keras.layers.Add()([input, conv2])\n","\n","    # Or, just don't do the residual connection if input and output dims don't\n","    # Match up. (This is how the official implementation is done)\n","    if input_channels == output_channels and stride == (1, 1):\n","      return tf.keras.layers.Add(name='Residual_'+block_id)([input, conv2])\n","    else:\n","      return conv2\n","\n","\n","# Input shape of CIFAR-10 image\n","inputs = tf.keras.layers.Input(shape=(32, 32, 3))\n","\n","# Initial 32-output channel convolution\n","conv1 = initial_conv2d()(inputs)\n","\n","# First MBConv series: (16, 16, 32) -> (16, 16, 16); t=1, c=16, n=1, s=1\n","b1a = mbconv2d(conv1, expansion_factor=1, output_channels=16, \n","                  stride=(1, 1), block_id='1a')\n","\n","# Second MBConv series: (16, 16, 16) -> (8, 8, 24); t=6, c=24, n=2, s=2\n","b2a = mbconv2d(b1a, expansion_factor=6, output_channels=24, \n","                  stride=(2, 2), block_id='2a')\n","b2b = mbconv2d(b2a, expansion_factor=6, output_channels=24, \n","                  stride=(1, 1), block_id='2b')\n","\n","# Third MBConv series: (8, 8, 24) -> (4, 4, 32); t=6, c=32, n=3, s=2\n","b3a = mbconv2d(b2b, expansion_factor=6, output_channels=32, \n","                  stride=(2, 2), block_id='3a')\n","b3b = mbconv2d(b3a, expansion_factor=6, output_channels=32, \n","                  stride=(1, 1), block_id='3b')\n","b3c = mbconv2d(b3b, expansion_factor=6, output_channels=32, \n","                  stride=(1, 1), block_id='3c')\n","\n","# Fourth MBConv series: (4, 4, 32) -> (2, 2, 64); t=6, c=64, n=4, s=2\n","b4a = mbconv2d(b3c, expansion_factor=6, output_channels=64, \n","                  stride=(2, 2), block_id='4a')\n","b4b = mbconv2d(b4a, expansion_factor=6, output_channels=64, \n","                  stride=(1, 1), block_id='4b')\n","b4c = mbconv2d(b4b, expansion_factor=6, output_channels=64, \n","                  stride=(1, 1), block_id='4c')\n","b4d = mbconv2d(b4c, expansion_factor=6, output_channels=64, \n","                  stride=(1, 1), block_id='4d')\n","\n","# Fifth MBConv series: (2, 2, 64) -> (2, 2, 96); t=6, c=96, n=3, s=1\n","b5a = mbconv2d(b4d, expansion_factor=6, output_channels=96, \n","                  stride=(1, 1), block_id='5a')\n","b5b = mbconv2d(b5a, expansion_factor=6, output_channels=96, \n","                  stride=(1, 1), block_id='5b')\n","b5c = mbconv2d(b5b, expansion_factor=6, output_channels=96, \n","                  stride=(1, 1), block_id='5c')\n","\n","# Sixth MBConv series: (2, 2, 96) -> (1, 1, 160); t=6, c=160, n=3, s=2\n","b6a = mbconv2d(b4d, expansion_factor=6, output_channels=160, \n","                  stride=(1, 1), block_id='6a')\n","b6b = mbconv2d(b5a, expansion_factor=6, output_channels=160, \n","                  stride=(1, 1), block_id='6b')\n","b6c = mbconv2d(b5b, expansion_factor=6, output_channels=160, \n","                  stride=(1, 1), block_id='6c')\n","\n","# Seventh MBConv series: (1, 1, 160) -> (1, 1, 320); t=6, c=320, n=3, s=1\n","b7a = mbconv2d(b4d, expansion_factor=6, output_channels=320, \n","                  stride=(1, 1), block_id='7a')\n","b7b = mbconv2d(b5a, expansion_factor=6, output_channels=320, \n","                  stride=(1, 1), block_id='7b')\n","b7c = mbconv2d(b5b, expansion_factor=6, output_channels=320, \n","                  stride=(1, 1), block_id='7c')\n","\n","# Final convolution\n","conv2 = final_conv2d()(b7c)\n","\n","# Dense layer and softmax\n","flatten = tf.keras.layers.Flatten()(conv2)\n","d2 = tf.keras.layers.Dense(10, activation=None)(flatten)\n","d2 = tf.keras.layers.BatchNormalization(momentum=0.999)(d2)\n","d2 = tf.keras.layers.Softmax()(d2)\n","\n","model = tf.keras.Model(inputs=inputs, outputs=d2)\n","\n","schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n","  0.045, 50000/BATCH_SIZE, 0.98, staircase=False, name='ExpDecaySchedule'\n",")\n","optimizer = tf.keras.optimizers.Adam(learning_rate=schedule)\n","model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', \n","              metrics=['accuracy'])\n","  \n","model.load_weights('mobileNetV2/model.ckpt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fYRs1zH7MCh0","executionInfo":{"status":"ok","timestamp":1639288484616,"user_tz":300,"elapsed":3447,"user":{"displayName":"Ryan Shue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg65rZPkz5bJ7ik0h6ITKz45vgUzj81potiaE77Dw=s64","userId":"11228421312565688614"}},"outputId":"6433f9ef-234d-4cd3-d89b-0cd3a88994fc"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f851fcf3210>"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["model.evaluate(test_x, test_y, verbose=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mz6iI0aPMam9","executionInfo":{"status":"ok","timestamp":1639288495557,"user_tz":300,"elapsed":6897,"user":{"displayName":"Ryan Shue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg65rZPkz5bJ7ik0h6ITKz45vgUzj81potiaE77Dw=s64","userId":"11228421312565688614"}},"outputId":"48fdb388-8748-40eb-e61e-fbcc6836a628"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["313/313 - 5s - loss: 1.4163 - accuracy: 0.6778 - 5s/epoch - 17ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["[1.4163216352462769, 0.6777999997138977]"]},"metadata":{},"execution_count":17}]}]}